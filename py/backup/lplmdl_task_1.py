# -*- coding: utf-8 -*-
"""LPLMDL Task 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m17xuC7pX8d1L_UPxTS4UUG3cElgiwyh

# Task 1: Upload and clean the data
The goal of this task is threefold:


1. we want to reduce the noise in the original raw text by removing everything that does not bring information to the language model. everything that is not exactly text: html tags, math equations, urls, etc
2. we want to prepare the corpus and make it ready for our language model by tokenizing the text.
3. And finally, we want to remove rows with short or very long texts. As you will see, some of the entries are mostly made of large numerical tables. Entries that are too long will not be good reflection of the corpus. Entris that are too short will not bring relevant information to the language model either.
"""

# We only need the following librairies

import pandas as pd
import re
import string
import csv

"""Let's load the dataset and shuffle it."""

data = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz', compression='gzip').sample(frac = 1, random_state = 0).reset_index(drop = True)

assert data.shape == (812132, 5), "The dataset does not have the right dimensions"

"""And start by exploring the dataset."""

data.head()

"""We have 3 types of text:"""

data.category.value_counts()

# example of titles
for p in data[data.category == 'title'].text.sample(3).values:
  print('-' * 20)
  print(p)

"""We see that posts text have html tags and latex formatted equations."""

for p in data[data.category == 'post'].text.sample(3).values:
  print('-' * 20)
  print(p)

# And here's a sample of comments
for p in data[data.category == 'comment'].text.sample(3).values:
  print('-' * 20)
  print(p)

"""# Clean up raw text
We're going to remove the following elements:
* html tags
* line returns
* urls
* latex equations
* numbers
* mentions: @someone
* digits
* most of the punctuation
* and extra spaces

For that we will use a series of simple regex patterns and the following pandas dataframe pattern:

```
pattern = r" some regex pattern"
df.text.apply(lambda t : re.sub(pattern,' ', t) )
```

Note that it's up to you to decide which elements should be removed or kept. This sequence of transformations can be modified.

Not also that the regex patterns we use here are chosen for their simplicity. Feel free to use more precise patterns.
"""

# remove html tags
data['text'] = data.text.apply(lambda t : re.sub("<[^>]*>",' ', t) )

# rm line returns
data['text'] = data.text.apply(lambda t : re.sub("[\r\n]+",' ', t) )

# rm urls
data['text'] = data.text.apply(lambda t : re.sub("http\S+",' ', t) )

# rm mentions
data['text'] = data.text.apply(lambda t : re.sub("@\S+",' ', t) )

# rm latex
data['text'] = data.text.apply(lambda t : re.sub("\$[^>]*\$",' ', t) )

# rm digits
data['text'] = data.text.apply(lambda t : re.sub("\d+",' ', t) )

# rm some of the punctuation but keep ,.!? and -
remove = '"#$%&()*+/:;<=>@[\\]^_`{|}~”“'
pattern = r"[{}]".format(remove)
data['text'] = data.text.apply(lambda t : re.sub(pattern,' ', t) )

# rm multiple spaces
data['text'] = data.text.apply(lambda t : re.sub("\s\s+",' ', t) )

# finally remove trailing spaces with strip()
data['text'] = data.text.apply(lambda t : t.strip() )

"""Let's check out the resulting text for the different types:"""

# titles should not be changed
for p in data[data.category == 'title'].text.sample(3).values:
  print('-' * 20)
  print(p)

# posts should have much less clutter
for p in data[data.category == 'post'].text.sample(3).values:
  print('-' * 20)
  print(p)

# comments should also be less noisy
for p in data[data.category == 'comment'].text.sample(3).values:
  print('-' * 20)
  print(p)

"""# Tokenize

Let's tokenize the text.
This will allow us to count the number of tokens of each text and subsequently remove test that are too long or too short.
You can use other librairies to tokenize the text (spacy for instance) or other tokenizer. Here we use the [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer) from NLTK.

And we create a new columns called tokens
"""

from nltk.tokenize import WordPunctTokenizer
tokenizer = WordPunctTokenizer()
data['tokens'] = data.text.apply(lambda t : tokenizer.tokenize(t.lower()))

"""Let's now count the tokens in each piece of text"""

data['n_tokens'] = data.tokens.apply(len)

data.n_tokens.describe()

data.n_tokens.hist(bins = 100)

"""We see that we have some extremely long texts. Let's look at the largest one"""

# this one has a very long series of "L,"
print(data[data.n_tokens > 10000].text.values[0])

"""We can see that most of the longest texts are composed of tables with limited semantic value.
We will remove rows that have more than an arbitrary number of tokens (let's say 5000) as well as rows that have too few tokens.
"""

data = data[(data.n_tokens > 4) & (data.n_tokens < 5000)].reset_index(drop = True)
data.shape

data.category.value_counts()

"""# Export data
We could export the dataframe as such using a pickle file format.

However if we want to keep the original csv format it's going to be easier if we transform the list of tokens into a space separated string.

On retrieval we will only have to split the string to get back the list of tokens.
"""

data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))
data.tokens.head()

"""
And finally let's export the dataframe into a csv file.
We will use that csv file as the new cleaned up and filtered out dataset to build our language model in task 2.
"""

data.to_csv('stackexchange_812k.tokenized.csv', quoting = csv.QUOTE_ALL, index = False)

"""# Conclusion
Removing or adding steps to this first text processing task will allow us to test different approaches in our language model building process.

For instance we can decide not to remove the latex formatted mathematical equation and see if the language model is able to create grammatically valid equations.

We could also implement a step to handle contractions (i'm, let's, ...) and see if that improves the quality of the generated text

Finally we could also decide to work on the vocabulary and filter out typos or non-English unknown words using named entity recognition to tag specific tokens.
"""
